{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Source Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'source', 'content', 'feature', 'title', 'url'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('/root/files/sql_result.csv')\n",
    "print(d.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only need the x:content and y:source\n",
    "## first check the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcc1eef803d447591203032af353d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib as  mpl\n",
    "source_cnt = d['source'].value_counts()\n",
    "s_cnt = dict(source_cnt)\n",
    "s_cnt = sorted(s_cnt.items(),key=lambda x:x[1],reverse=True)[:10]\n",
    "sc = [i[0] for i in s_cnt]\n",
    "cnt = [i[1] for i in s_cnt]\n",
    "plt.bar(sc,cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only take the top 10 lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'delete 0'\n",
      "{'中国台湾网': 7,\n",
      " '中国新闻网': 3,\n",
      " '中国证券报?中证网': 2,\n",
      " '南方日报第01版': 6,\n",
      " '参考消息网': 4,\n",
      " '央广网': 8,\n",
      " '微博': 1,\n",
      " '新华社': 0,\n",
      " '新华网': 9,\n",
      " '环球网': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新华社</td>\n",
       "      <td>这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\n</td>\n",
       "      <td>{\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...</td>\n",
       "      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）</td>\n",
       "      <td>http://world.huanqiu.com/hot/2017-06/10866126....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89612</td>\n",
       "      <td>张怡</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>受到A股被纳入MSCI指数的利好消息刺激，A股市场从周三开始再度上演龙马行情，周四上午金...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>金融股一枝独秀 配置价值犹存</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89610</td>\n",
       "      <td>申玉彬 整理</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>沙漠雄鹰：震荡有利消化套牢筹码\\n　　周四开盘上证50在银行券商大蓝筹带动下一度涨近2%...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>博友早评：震荡有利消化套牢筹码</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>89608</td>\n",
       "      <td>吴瞬</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>6月21日，A股纳入MSCI指数尘埃落定，但当天被寄予厚望的券商股并未扛起反弹大旗。22...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>纳入MSCI指数 A股长期配置价值提升</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>中国新闻网</td>\n",
       "      <td>中新网6月19日电 据外媒报道，美国底特律一名男子1976年因为一根头发被定谋杀罪，监禁41...</td>\n",
       "      <td>{\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...</td>\n",
       "      <td>因为犯罪现场的一根头发，他坐冤狱41年后终获释</td>\n",
       "      <td>http://world.huanqiu.com/hot/2017-06/10866136....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  author     source  \\\n",
       "3   89614     NaN        新华社   \n",
       "5   89612      张怡  中国证券报?中证网   \n",
       "7   89610  申玉彬 整理  中国证券报?中证网   \n",
       "9   89608      吴瞬  中国证券报?中证网   \n",
       "12  89605     NaN      中国新闻网   \n",
       "\n",
       "                                              content  \\\n",
       "3       这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\n   \n",
       "5   　　受到A股被纳入MSCI指数的利好消息刺激，A股市场从周三开始再度上演龙马行情，周四上午金...   \n",
       "7   　　沙漠雄鹰：震荡有利消化套牢筹码\\n　　周四开盘上证50在银行券商大蓝筹带动下一度涨近2%...   \n",
       "9   　　6月21日，A股纳入MSCI指数尘埃落定，但当天被寄予厚望的券商股并未扛起反弹大旗。22...   \n",
       "12  中新网6月19日电 据外媒报道，美国底特律一名男子1976年因为一根头发被定谋杀罪，监禁41...   \n",
       "\n",
       "                                              feature  \\\n",
       "3   {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
       "5   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "7   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "9   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "12  {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
       "\n",
       "                            title  \\\n",
       "3   葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）   \n",
       "5                  金融股一枝独秀 配置价值犹存   \n",
       "7                 博友早评：震荡有利消化套牢筹码   \n",
       "9             纳入MSCI指数 A股长期配置价值提升   \n",
       "12        因为犯罪现场的一根头发，他坐冤狱41年后终获释   \n",
       "\n",
       "                                                  url  \n",
       "3   http://world.huanqiu.com/hot/2017-06/10866126....  \n",
       "5   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "7   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "9   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "12  http://world.huanqiu.com/hot/2017-06/10866136....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = d.content.tolist()\n",
    "sc_labels_t = {s:i for i,s in enumerate(sc)}\n",
    "total = len(d)\n",
    "fd = d[d['source'].isin(set(sc))]\n",
    "pp(f'delete {total - len(content)}')\n",
    "pp(sc_labels_t)\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = fd.content.tolist()\n",
    "content = [str(i).replace('\\n','').replace('\\u3000','') for i in content]\n",
    "sc_labels = fd.source.apply(lambda x:sc_labels_t[x]).tolist()\n",
    "\n",
    "data = [[c,s] for c,s in zip(content,sc_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(s):\n",
    "    return list(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 concurrent 模块多进程处理数据\n",
    "由于多进程处理数据无法保证处理顺序，故 x和 y 对应不上，这里将 x 和 y 包装好后进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.847 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.827 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.854 seconds.\n",
      "Loading model cost 0.881 seconds.\n",
      "Loading model cost 0.863 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.892 seconds.\n",
      "Loading model cost 0.843 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.875 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.911 seconds.\n",
      "Loading model cost 0.879 seconds.\n",
      "Loading model cost 0.898 seconds.\n",
      "Loading model cost 0.907 seconds.\n",
      "Loading model cost 0.884 seconds.\n",
      "Loading model cost 0.915 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.893 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.954 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.972 seconds.\n",
      "Loading model cost 0.920 seconds.\n",
      "Loading model cost 0.951 seconds.\n",
      "Loading model cost 0.946 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 23.8 s, total: 32.1 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent import futures\n",
    "def multi_process(func,d):\n",
    "    ret = []\n",
    "    def chunks(arr, n):\n",
    "        return [arr[i:i+n] for i in range(0, len(arr), n)]\n",
    "    d = chunks(d,20)\n",
    "    with futures.ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        f = [executor.submit(func, item) for item in d]\n",
    "        for future in futures.as_completed(f):\n",
    "            ret.append(future.result())\n",
    "    return sum(ret,[]) \n",
    "\n",
    "def cut_content(c):\n",
    "    return [[cut(i[0]),i[1]] for i in c]\n",
    "\n",
    "T = multi_process(cut_content,data)\n",
    "cut_content = [t[0] for t in T]\n",
    "y = [t[1] for t in T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design feature by content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83774"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216071\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for c in cut_content: cnt.update(c)\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the tokens by min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n"
     ]
    }
   ],
   "source": [
    "min_count = 10\n",
    "tokens = set(filter(lambda x:x[1] > min_count,dict(cnt).items()))\n",
    "tokens = [i[0] for i in tokens]\n",
    "print(len(tokens))\n",
    "w2i = {k:v for v,k in enumerate(tokens)}\n",
    "i2w = {v:k for k,v in w2i.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "def calculate_term_frequency(c):\n",
    "    t = []\n",
    "    for i in c:\n",
    "        cnt = Counter(i[0])\n",
    "        tf = np.zeros((len(tokens)))\n",
    "        for k,v in dict(cnt).items():\n",
    "            if k not in tokens:\n",
    "                continue\n",
    "            tf[w2i[k]] = v / len(i[0])\n",
    "        t.append([tf,i[1]])\n",
    "    return t\n",
    "tf = multi_process(calculate_term_frequency,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse document frequency\n",
    "idf = $lg\\frac{|D|}{\\{j:t_i belongs d_j\\}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inverse_document_frequency(cut_sentences):\n",
    "    idf = {}\n",
    "    for c in cut_sentences:\n",
    "        cache = set()\n",
    "        for k in c:\n",
    "            if k in cache:\n",
    "                continue\n",
    "            idf[k] = idf.get(k,0) + 1\n",
    "            cache.add(k)\n",
    "    return {k : len(cut_content) / v for k,v in idf.items()}\n",
    "idf = calculate_inverse_document_frequency(cut_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 64.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9692513368983957"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = [],[]\n",
    "for i in tf:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "print(len(x_train))\n",
    "lr = LogisticRegression(n_jobs=64)\n",
    "lr.fit(x_train,y_train)\n",
    "lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     19671\n",
      "           1       0.99      1.00      0.99       631\n",
      "           2       0.00      0.00      0.00       153\n",
      "           3       0.00      0.00      0.00       125\n",
      "           4       0.00      0.00      0.00        82\n",
      "           5       0.00      0.00      0.00        73\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00        53\n",
      "           8       0.00      0.00      0.00        41\n",
      "           9       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     20944\n",
      "   macro avg       0.20      0.20      0.20     20944\n",
      "weighted avg       0.94      0.97      0.95     20944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = lr.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'中国台湾网': 7,\n",
      " '中国新闻网': 3,\n",
      " '中国证券报?中证网': 2,\n",
      " '南方日报第01版': 6,\n",
      " '参考消息网': 4,\n",
      " '央广网': 8,\n",
      " '微博': 1,\n",
      " '新华社': 0,\n",
      " '新华网': 9,\n",
      " '环球网': 5}\n"
     ]
    }
   ],
   "source": [
    "pp(sc_labels_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'你': 1.0,\n",
      "  '回事': 1.4054651081081644,\n",
      "  '小子': 2.8109302162163288,\n",
      "  '怎么': 1.4054651081081644},\n",
      " {'你': 1.0, '能': 1.4054651081081644, '行吗': 1.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "cc = [\n",
    "    ['你','小子','怎么','回事','小子'],\n",
    "    ['能','行吗','你']\n",
    "]\n",
    "def calculate_inverse_document_frequency(cut_sentences):\n",
    "    idf = {}\n",
    "    for c in cut_sentences:\n",
    "        cache = set()\n",
    "        for k in c:\n",
    "            if k in cache:\n",
    "                continue\n",
    "            idf[k] = idf.get(k,0) + 1\n",
    "            cache.add(k)\n",
    "    return {k : np.log((len(cut_sentences)+1) / (v+1)) + 1 for k,v in idf.items()}\n",
    "\n",
    "def calculate_term_frequency(cut_sentences):\n",
    "    _tokens = set()\n",
    "    for c in cut_sentences:\n",
    "        _tokens.update(c)\n",
    "    tf = []\n",
    "    for c in cut_sentences:\n",
    "        cnt = Counter(c)\n",
    "        tf.append({k:v for k,v in cnt.items()})\n",
    "    return list(_tokens),tf\n",
    "\n",
    "\n",
    "def calculate_tfidf(cut_sentences,norm='l1'):\n",
    "    def normalize(v):\n",
    "        e = {\n",
    "            'l1':1,\n",
    "            'l2':2\n",
    "        }.get(norm)\n",
    "        return [k / sum(map(lambda x:x**e,v)) for k in v]\n",
    "    idf = calculate_inverse_document_frequency(cut_sentences)\n",
    "    tokens,tf = calculate_term_frequency(cut_sentences)\n",
    "    vocab = {v:k for k,v in enumerate(tokens)}\n",
    "    tfidf = []\n",
    "    for i in range(len(tf)):\n",
    "        tfidf.append({k:v * idf[k] for k,v in tf[i].items()})\n",
    "    pp(tfidf)\n",
    "calculate_tfidf(cc)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '回事', '小子', '怎么', '能', '行吗']\n",
      "[[1.         1.40546511 2.81093022 1.40546511 0.         0.        ]\n",
      " [1.         0.         0.         0.         1.40546511 1.40546511]]\n"
     ]
    }
   ],
   "source": [
    "cc = [\n",
    "    '你 小子 怎么 回事 小子',\n",
    "    '能 行吗 你'\n",
    "]\n",
    "vector = TfidfVectorizer(smooth_idf=True,norm=None, token_pattern='\\\\b\\\\w+\\\\b') \n",
    "tfidf = vector.fit_transform(cc) # 得到结果\n",
    "wordlist = vector.get_feature_names()#获取词袋模型中的所有词  \n",
    "print(wordlist)\n",
    "weightlist = tfidf.toarray()  \n",
    "print(weightlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How sklearn works\n",
    "if smooth_idf = True,then \n",
    "$$\n",
    "idf_w = log\\frac{|D| + 1}{|w| + 1}\n",
    "$$\n",
    "if smooth_idf = False, then\n",
    "$$\n",
    "idf_w = log\\frac{|D| + 1}{|w| + 1}\n",
    "$$\n",
    "维基百科中说的平滑 idf 是分子加 1，分母不加 1.这样是有点小问题的，假设词 w 在所有文档中都出现过，那么 idf 就为\n",
    "$$\n",
    "idf_w = log\\frac{|D|}{|D| + 1}\n",
    "$$\n",
    "and then idf becomes negative.\n",
    "One more thing, sklearn add 1 to all idf. Why ???\n",
    "\n",
    "sklearn use $e$ as log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
