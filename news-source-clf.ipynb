{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Source Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'source', 'content', 'feature', 'title', 'url'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('/root/files/sql_result.csv')\n",
    "print(d.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only need the x:content and y:source\n",
    "## first check the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a01775fe934c698a79659f98304416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "source_cnt = d['source'].value_counts()\n",
    "s_cnt = dict(source_cnt)\n",
    "s_cnt = sorted(s_cnt.items(),key=lambda x:x[1],reverse=True)[:10]\n",
    "sc = [i[0] for i in s_cnt]\n",
    "cnt = [i[1] for i in s_cnt]\n",
    "plt.bar(sc,cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only take the top 10 lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'delete 0'\n",
      "{'中国台湾网': 7,\n",
      " '中国新闻网': 3,\n",
      " '中国证券报?中证网': 2,\n",
      " '南方日报第01版': 6,\n",
      " '参考消息网': 4,\n",
      " '央广网': 8,\n",
      " '微博': 1,\n",
      " '新华社': 0,\n",
      " '新华网': 9,\n",
      " '环球网': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新华社</td>\n",
       "      <td>这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\n</td>\n",
       "      <td>{\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...</td>\n",
       "      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）</td>\n",
       "      <td>http://world.huanqiu.com/hot/2017-06/10866126....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89612</td>\n",
       "      <td>张怡</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>受到A股被纳入MSCI指数的利好消息刺激，A股市场从周三开始再度上演龙马行情，周四上午金...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>金融股一枝独秀 配置价值犹存</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89610</td>\n",
       "      <td>申玉彬 整理</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>沙漠雄鹰：震荡有利消化套牢筹码\\n　　周四开盘上证50在银行券商大蓝筹带动下一度涨近2%...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>博友早评：震荡有利消化套牢筹码</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>89608</td>\n",
       "      <td>吴瞬</td>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>6月21日，A股纳入MSCI指数尘埃落定，但当天被寄予厚望的券商股并未扛起反弹大旗。22...</td>\n",
       "      <td>{\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...</td>\n",
       "      <td>纳入MSCI指数 A股长期配置价值提升</td>\n",
       "      <td>http://www.cs.com.cn/gppd/201706/t20170623_533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>中国新闻网</td>\n",
       "      <td>中新网6月19日电 据外媒报道，美国底特律一名男子1976年因为一根头发被定谋杀罪，监禁41...</td>\n",
       "      <td>{\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...</td>\n",
       "      <td>因为犯罪现场的一根头发，他坐冤狱41年后终获释</td>\n",
       "      <td>http://world.huanqiu.com/hot/2017-06/10866136....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  author     source  \\\n",
       "3   89614     NaN        新华社   \n",
       "5   89612      张怡  中国证券报?中证网   \n",
       "7   89610  申玉彬 整理  中国证券报?中证网   \n",
       "9   89608      吴瞬  中国证券报?中证网   \n",
       "12  89605     NaN      中国新闻网   \n",
       "\n",
       "                                              content  \\\n",
       "3       这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\n   \n",
       "5   　　受到A股被纳入MSCI指数的利好消息刺激，A股市场从周三开始再度上演龙马行情，周四上午金...   \n",
       "7   　　沙漠雄鹰：震荡有利消化套牢筹码\\n　　周四开盘上证50在银行券商大蓝筹带动下一度涨近2%...   \n",
       "9   　　6月21日，A股纳入MSCI指数尘埃落定，但当天被寄予厚望的券商股并未扛起反弹大旗。22...   \n",
       "12  中新网6月19日电 据外媒报道，美国底特律一名男子1976年因为一根头发被定谋杀罪，监禁41...   \n",
       "\n",
       "                                              feature  \\\n",
       "3   {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
       "5   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "7   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "9   {\"type\":\"市场\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
       "12  {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
       "\n",
       "                            title  \\\n",
       "3   葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）   \n",
       "5                  金融股一枝独秀 配置价值犹存   \n",
       "7                 博友早评：震荡有利消化套牢筹码   \n",
       "9             纳入MSCI指数 A股长期配置价值提升   \n",
       "12        因为犯罪现场的一根头发，他坐冤狱41年后终获释   \n",
       "\n",
       "                                                  url  \n",
       "3   http://world.huanqiu.com/hot/2017-06/10866126....  \n",
       "5   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "7   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "9   http://www.cs.com.cn/gppd/201706/t20170623_533...  \n",
       "12  http://world.huanqiu.com/hot/2017-06/10866136....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = d.content.tolist()\n",
    "sc_labels_t = {s:i for i,s in enumerate(sc)}\n",
    "total = len(d)\n",
    "fd = d[d['source'].isin(set(sc))]\n",
    "pp(f'delete {total - len(content)}')\n",
    "pp(sc_labels_t)\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = fd.content.tolist()\n",
    "content = [str(i).replace('\\n','').replace('\\u3000','') for i in content]\n",
    "sc_labels = fd.source.apply(lambda x:sc_labels_t[x]).tolist()\n",
    "\n",
    "data = [[c,s] for c,s in zip(content,sc_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(s): return list(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 concurrent 模块多进程处理数据\n",
    "由于多进程处理数据无法保证处理顺序，故 x和 y 对应不上，这里将 x 和 y 包装好后进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.777 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.826 seconds.\n",
      "Loading model cost 0.823 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.819 seconds.\n",
      "Loading model cost 0.821 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.824 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.825 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.820 seconds.\n",
      "Loading model cost 0.823 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.829 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.824 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.838 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.828 seconds.\n",
      "Loading model cost 0.862 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.885 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.903 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.891 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.031 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.059 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 3.55 s, total: 13.6 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent import futures\n",
    "def multi_process(func,d):\n",
    "    ret = []\n",
    "    def chunks(arr, n):\n",
    "        return [arr[i:i+n] for i in range(0, len(arr), n)]\n",
    "    d = chunks(d,20)\n",
    "    with futures.ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        f = [executor.submit(func, item) for item in d]\n",
    "        for future in futures.as_completed(f):\n",
    "            ret.append(future.result())\n",
    "    return sum(ret,[]) \n",
    "\n",
    "def cut_content(c):\n",
    "    return [[cut(i[0]),i[1]] for i in c]\n",
    "\n",
    "T = multi_process(cut_content,data)\n",
    "cut_content = [t[0] for t in T]\n",
    "y = [t[1] for t in T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design feature by content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cc = [\n",
    "    ['你','小子','怎么','回事','小子'],\n",
    "    ['能','行吗','你']\n",
    "]\n",
    "def calculate_inverse_document_frequency(cut_sentences):\n",
    "    idf = {}\n",
    "    for c in cut_sentences:\n",
    "        cache = set()\n",
    "        for k in c:\n",
    "            if k in cache:\n",
    "                continue\n",
    "            idf[k] = idf.get(k,0) + 1\n",
    "            cache.add(k)\n",
    "    return {k : np.log((len(cut_sentences)+1) / (v+1)) + 1 for k,v in idf.items()}\n",
    "\n",
    "def calculate_term_frequency(cut_sentences):\n",
    "    _tokens = set()\n",
    "    for c in cut_sentences:\n",
    "        _tokens.update(c)\n",
    "    tf = []\n",
    "    for c in cut_sentences:\n",
    "        cnt = Counter(c)\n",
    "        tf.append({k:v for k,v in cnt.items()})\n",
    "    return list(_tokens),tf\n",
    "\n",
    "\n",
    "def calculate_tfidf(cut_sentences,norm='l1'):\n",
    "    def normalize(v):\n",
    "        e = {\n",
    "            'l1':1,\n",
    "            'l2':2\n",
    "        }.get(norm)\n",
    "        return [k / sum(map(lambda x:x**e,v)) for k in v]\n",
    "    idf = calculate_inverse_document_frequency(cut_sentences)\n",
    "    tokens,tf = calculate_term_frequency(cut_sentences)\n",
    "    vocab = {v:k for k,v in enumerate(tokens)}\n",
    "    tfidf = []\n",
    "    for i in range(len(tf)):\n",
    "        tfidf.append({k:v * idf[k] for k,v in tf[i].items()})\n",
    "    return tfidf,vocab\n",
    "\n",
    "def transfer_to_sparse_matrix(tfidf,vocab):\n",
    "    # first new zeros matrix\n",
    "    import scipy.sparse as sparse\n",
    "    n = np.zeros((len(tfidf),len(vocab)))\n",
    "    for i,t in enumerate(tfidf):\n",
    "        for k,v in t.items():\n",
    "            n[i][vocab[k]] = v\n",
    "    return  sparse.coo_matrix(n)\n",
    " \n",
    "tfidf,vocab = calculate_tfidf(cut_content)\n",
    "x = transfer_to_sparse_matrix(tfidf,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83774, 216071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62830, 216071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 64.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9933155080213903"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "print(x_train.shape)\n",
    "lr = LogisticRegression(n_jobs=64)\n",
    "lr.fit(x_train,y_train)\n",
    "lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19657\n",
      "           1       0.98      1.00      0.99       636\n",
      "           2       0.92      0.88      0.90       133\n",
      "           3       0.83      0.87      0.85       126\n",
      "           4       0.94      0.98      0.96        97\n",
      "           5       0.86      0.79      0.82        70\n",
      "           6       0.89      0.63      0.73        75\n",
      "           7       0.95      0.98      0.97        60\n",
      "           8       0.97      0.57      0.72        51\n",
      "           9       0.46      0.31      0.37        39\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     20944\n",
      "   macro avg       0.88      0.80      0.83     20944\n",
      "weighted avg       0.99      0.99      0.99     20944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How sklearn works\n",
    "if smooth_idf = True,then \n",
    "$$\n",
    "idf_w = log\\frac{|D| + 1}{|w| + 1}\n",
    "$$\n",
    "if smooth_idf = False, then\n",
    "$$\n",
    "idf_w = log\\frac{|D| + 1}{|w| + 1}\n",
    "$$\n",
    "维基百科中说的平滑 idf 是分子加 1，分母不加 1.这样是有点小问题的，假设词 w 在所有文档中都出现过，那么 idf 就为\n",
    "$$\n",
    "idf_w = log\\frac{|D|}{|D| + 1}\n",
    "$$\n",
    "and then idf becomes negative.\n",
    "One more thing, sklearn add 1 to all idf. Why ???\n",
    "\n",
    "sklearn use $e$ as log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '回事', '小子', '怎么', '能', '行吗']\n",
      "[[1.         1.40546511 2.81093022 1.40546511 0.         0.        ]\n",
      " [1.         0.         0.         0.         1.40546511 1.40546511]]\n"
     ]
    }
   ],
   "source": [
    "cc = [\n",
    "    '你 小子 怎么 回事 小子',\n",
    "    '能 行吗 你'\n",
    "]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector = TfidfVectorizer(smooth_idf=True,norm=None, token_pattern='\\\\b\\\\w+\\\\b') \n",
    "tfidf = vector.fit_transform(cc) # 得到结果\n",
    "wordlist = vector.get_feature_names()#获取词袋模型中的所有词  \n",
    "print(wordlist)\n",
    "weightlist = tfidf.toarray()  \n",
    "print(weightlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
